{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 강화학습\n",
    "- 에이전트: 인공지능 플레이어\n",
    "- 환경: 에이전트가 솔루션을 찾기 위한 무대\n",
    "- 행동: 에이전트가 환경 안에서 시행하는 상호작용\n",
    "- 보상: 에이전트의 행동에 따른 점수 혹은 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN의 주요 특징\n",
    "- 기억하기 & 다시보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: Pillow<=7.2.0 in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from gym) (7.2.0)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: future in c:\\users\\hwj43\\anaconda3\\lib\\site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "!pip install gym\n",
    "sys.path.append(\"c:/users/hwj43/anaconda3/lib/site-packages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gym : 카트풀 등 여러 게임 환경을 제공하는 패키지\n",
    "# deque : 먼저 들어온 데이터가 먼저 나가게 되는 큐(queue) [FIFO]\n",
    "#         deque는 double-ended queue의 약자로 큐와는 다르게 양쪽 끝에서 삽입과 삭제가 모두 가능\n",
    "# random : 에이전트가 무작위로 행동할 확률을 구하기 위해 사용하는 파이썬의 기본 패키지\n",
    "# math : 에이전트가 무작위로 행동할 확률을 구하기 위해 사용하는 파이썬의 기본 패키지\n",
    "import gym\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n에피소드는 총 플레이 할 게임의 수  \\nEPS는 무작위로 행동할 확률로 모든 행동을 경험할 수 있도록 해줌(90%부터 시작해서 마지막엔 5%까지 떨어짐)\\nEPS_DECAY는 90에서 5로 떨어지는 감소율\\nGAMMA는 에이전트가 현재 보상을 미래 보상보다 얼마나 가치있게 여기는지이다.(할인계수의 개념)  \\n지금 받은 만원 말고, 1년 뒤에 받을 만원은 이자율만큼 곱해주어야함\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyper-parameter\n",
    "EPISODES = 50      # 에피소드 반복 횟수 (총 플레이할 게임의 수)\n",
    "EPS_START = 0.9    # 학습 시작 시 에이전트가 무작위로 행동할 확률 \n",
    "EPS_END = 0.05     # 학습 막바지에 에이전트가 무작위로 행동할 확률\n",
    "EPS_DECAY = 200    # 학습 진행 시 에이전트가 무작위로 행동할 확률을 감소시키는 값\n",
    "GAMMA = 0.8        # 할인계수\n",
    "LR = 0.001         # 학습률\n",
    "BATCH_SIZE = 64    # 배치크기\n",
    "\n",
    "'''\n",
    "에피소드는 총 플레이 할 게임의 수  \n",
    "EPS는 무작위로 행동할 확률로 모든 행동을 경험할 수 있도록 해줌(90%부터 시작해서 마지막엔 5%까지 떨어짐)\n",
    "EPS_DECAY는 90에서 5로 떨어지는 감소율\n",
    "GAMMA는 에이전트가 현재 보상을 미래 보상보다 얼마나 가치있게 여기는지이다.(할인계수의 개념)  \n",
    "지금 받은 만원 말고, 1년 뒤에 받을 만원은 이자율만큼 곱해주어야함\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력\n",
    "- 카트 위치\n",
    "- 카트 속도\n",
    "- 막대기 각도\n",
    "- 막대기 속도\n",
    "\n",
    "### 출력\n",
    "- 0 (왼쪽)\n",
    "- 1 (오른쪽)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MEMORY\n",
    "\n",
    "- 딥러닝 모델들은 보통 학습 데이터 샘플이 독립적이라 가정하지만, 강화학습에서는 연속된 상태가 강한 상관관계를 가지고 있음\n",
    "    - 무작위로 가지고 오지않고 연속적인 경험을 학습하게 된다면 **초반의 몇가지 경험패턴**에만 치중해서 학습하게 됨\n",
    "- 두번째는 신경망이 새로운 경험을 전 경험에 겹쳐 쓰며 쉽게 잊어버림\n",
    "### \"기억하기\" 기능 추가  \n",
    "\n",
    "  \n",
    "- **이전 경험들을 배열에 담아 계속 재학습**시키면 신경망이 잊지 않게 한다는 아이디어\n",
    "- 기억한 경험들은 학습을 할 때 무작위로 뽑아 경험간의 상관관계를 줄인다  \n",
    "\n",
    "- 각 경험은 상태, 행동, 보상등을 담아야 함\n",
    "    - 이전 경험들에 관한 기억을 담고자 **memory라는 배열**을 만든다\n",
    "    \n",
    "```python\n",
    "self.memory = [(상태, 행동, 보상, 다음상태)...]\n",
    "```\n",
    "\n",
    "복잡한 모델을 만들때는 memory를 클래스로 구현하기도 하지만, 이번예제에서는 사용하기 가장 간단한 큐(queue) 자료구조 사용  \n",
    "파이썬에서의 deque의 maxlen을 지정해주면 큐가 가득 찼을때 제일 오래된 요소부터 없애줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        # 4개의 입력, 2개의 출력\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(4, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256,2))\n",
    "        \n",
    "        #optimizer\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
    "        \n",
    "        #학습을 반복할 때마다 증가하는 변수\n",
    "        self.steps_done = 0\n",
    "\n",
    "        self.memory = deque(maxlen = 10000)\n",
    "   \n",
    "    #self.memory 배열에 새로운 경험을 덧붙일 memorize() 함수를 만듦\n",
    "    #memorize() 함수는 self.memory 배열에 현재상태(state), 현재 상태에서 한 행동(action), 행동에 대한 보상(reward), 행동으로 인해 생성된 상태(next_state)\n",
    "    def memorize(self, state, action, reward, next_state):\n",
    "        self.memory.append((state, action,\n",
    "                            torch.FloatTensor([reward]),\n",
    "                           torch.FloatTensor([next_state])))\n",
    "        \n",
    "        \n",
    "    #앱실론의 값이 크면 신경망 학습하여 행동하는 쪽으로, 낮으면 무작위로 행동\n",
    "    #이 알고리즘을 epsilon-greedy 알고리즘이라고함\n",
    "    def act(self, state):\n",
    "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
    "        self.steps_done += 1 #학습 진행 될때마다 +1\n",
    "        if random.random() > eps_threshold:\n",
    "            return self.model(state).data.max(1)[1].view(1,1)\n",
    "        else:\n",
    "            return torch.LongTensor([[random.randrange(2)]])\n",
    "    \n",
    "    #경험으로부터 배우기\n",
    "    #에이전트가 기억하고 다시 상기하는 과정(experience replay)\n",
    "    #self.memory에 저장된 경험들의 수가 아직 배치 크기(BATCH_SIZE) 보다 커질 때까진 return으로 학습을 거르고, 만약 경험이 충분히 쌓이면 self.memory 큐에서 무작위로 배치 크기만큼의 '경험'들 가지고 오기\n",
    "    # 경험들을 무작위로 가지고오면 각 경험 샘플간의 상관성 줄이기 가능\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return\n",
    "        batch = random.sample(self.memory, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states = zip(*batch)\n",
    "        \n",
    "        states = torch.cat(states)\n",
    "        actions = torch.cat(actions)\n",
    "        rewards = torch.cat(rewards)\n",
    "        next_states = torch.cat(next_states)\n",
    "        \n",
    "        current_q = self.model(states).gather(1, actions)\n",
    "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
    "        expected_q = rewards + (GAMMA * max_next_q)\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습 준비하기\n",
    "env = gym.make('CartPole-v0') # 환경 만들어주기\n",
    "agent = DQNAgent() #에이전트\n",
    "score_history = []  #점수 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에피소드:1 점수:28\n",
      "에피소드:2 점수:16\n",
      "에피소드:3 점수:10\n",
      "에피소드:4 점수:9\n",
      "에피소드:5 점수:14\n",
      "에피소드:6 점수:11\n",
      "에피소드:7 점수:13\n",
      "에피소드:8 점수:13\n",
      "에피소드:9 점수:11\n",
      "에피소드:10 점수:12\n",
      "에피소드:11 점수:13\n",
      "에피소드:12 점수:15\n",
      "에피소드:13 점수:11\n",
      "에피소드:14 점수:10\n",
      "에피소드:15 점수:13\n",
      "에피소드:16 점수:17\n",
      "에피소드:17 점수:14\n",
      "에피소드:18 점수:18\n",
      "에피소드:19 점수:9\n",
      "에피소드:20 점수:11\n",
      "에피소드:21 점수:10\n",
      "에피소드:22 점수:12\n",
      "에피소드:23 점수:12\n",
      "에피소드:24 점수:10\n",
      "에피소드:25 점수:14\n",
      "에피소드:26 점수:13\n",
      "에피소드:27 점수:15\n",
      "에피소드:28 점수:12\n",
      "에피소드:29 점수:30\n",
      "에피소드:30 점수:14\n",
      "에피소드:31 점수:18\n",
      "에피소드:32 점수:56\n",
      "에피소드:33 점수:43\n",
      "에피소드:34 점수:45\n",
      "에피소드:35 점수:48\n",
      "에피소드:36 점수:101\n",
      "에피소드:37 점수:200\n",
      "에피소드:38 점수:150\n",
      "에피소드:39 점수:88\n",
      "에피소드:40 점수:170\n",
      "에피소드:41 점수:190\n",
      "에피소드:42 점수:165\n",
      "에피소드:43 점수:176\n",
      "에피소드:44 점수:200\n",
      "에피소드:45 점수:185\n",
      "에피소드:46 점수:200\n",
      "에피소드:47 점수:200\n",
      "에피소드:48 점수:200\n",
      "에피소드:49 점수:200\n",
      "에피소드:50 점수:200\n"
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "for e in range(1, EPISODES +1): #얼마나 많은 게임을 진행하느냐\n",
    "    state = env.reset() #게임을 시작할때마다 cartpole 게임환경의 상태를 초기화\n",
    "    steps = 0  \n",
    "    \n",
    "    while True: \n",
    "        env.render() #게임 화면을 띄움\n",
    "        \n",
    "        state = torch.FloatTensor([state]) #현재 게임의 상태 state를 텐서로 만듦\n",
    "        action = agent.act(state) #에이전트의 행동함수 act()의 입력으로 사용\n",
    "        \n",
    "        next_state, reward, done, _  = env.step(action.item()) \n",
    "        #action 변수는 파이토치 텐서. \n",
    "        #item()함수로 에이전트가 한 행동의 번호를 추출하여 step()함수에 입력해주면 에이전트의 행동에 따른 다음 상태(next_state), 보상(reward), 그리고 종료여부(done) 출력\n",
    "        \n",
    "        \n",
    "        #게임이 끝났을 경우 마이너스 보상 주기\n",
    "        if done:\n",
    "            reward = -1 # 막대가 넘어져서 게임이 끝났을 경우\n",
    "        agent.memorize(state, action, reward, next_state) # 이 경험을 기억\n",
    "        agent.learn()\n",
    "        \n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        \n",
    "        \n",
    "        # 게임이 끝나면 done이 True 가 된다\n",
    "        if done:\n",
    "            print(\"에피소드:{} 점수:{}\".format(e, steps))\n",
    "            score_history.append(steps)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수 기록을 그래프로 그려서 시각화\n",
    "plt.plot(score_history)\n",
    "plt.ylabel('score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
